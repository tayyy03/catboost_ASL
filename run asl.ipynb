{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import keyboard\n",
    "import pygame\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Sentence: ZZZZZZZZZZZCCCCCCEEEEEEEEESSSSASSSSASSASSSAAASAAAASASSSSASAAASAASAAASSSSSSSSSSSSESEESSEESSSSSSSSSSEEEEESSEESEEEEEEJJJZJJEEZJEEEJJJZZEJJZJZZZJJSZZZSZNSSSSSSSAEEEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASASEEEESEESSESSSSSEEEEEEESSSESEEESSSSSSXZZZZZZZZJZZJZJZZZZZZZZZZZZZZRZZ0NNNNNN0N00NXNNNNN00NNNNNNNNNNNNNNNNNNNNNNNNNNN00XX0NNNN000NX600000XZZNZNNXZ6XZZ666Z6ZZ6XZXZ66ZXXXZXXNNNNNNNNZ6Z66MJ66Z6Z6ZZZNZ44RRRRRRBBBBBBBBBBBBBBBBBBBBBBBBBBBBEEEESSAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANANAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZAZZZZAAZZZZABN AZZZZAAZZZZZSSSSEEEESASAAAAAAAAAAAAAAAASAAAAA AAAAAAAAAAASAAASSS AAAAAAAAAASASSSSSSSSSSSAAASSSSAASAASSSASAANNNNNNNNNNNNNNNSAAAAAAAAAAAAAAAAAAAAAAAAAASSSEEEEEEEEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSEEFEB55555LLLLLLLZLZEEECCCCC000000000000000000000000000000000000ZRRRRRRRRRRRRRRRRRRURRUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUASU UUUUUUUUUURRRRR\n",
      "Selected Label: \n"
     ]
    }
   ],
   "source": [
    "# Initialize Pygame for sound effects\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load the model from the same directory as the input file\n",
    "model_path = os.path.join(\"D:/dataset/viscom/TA\", \"asl_alphabet_CatBoostClassifier.pkl\")\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "DRAW_HANDS = True\n",
    "\n",
    "# Map predicted labels to corresponding text\n",
    "label_to_text = {\n",
    "    'A': 'A',\n",
    "    'B': 'B',\n",
    "    'C': 'C',\n",
    "    'D': 'D',\n",
    "    'E': 'E',\n",
    "    'F': 'F',\n",
    "    'G': 'G',\n",
    "    'H': 'H',\n",
    "    'I': 'I',\n",
    "    'J': 'J',\n",
    "    'K': 'K',\n",
    "    'L': 'L',\n",
    "    'M': 'M',\n",
    "    'N': 'N',\n",
    "    'O': 'O',\n",
    "    'P': 'P',\n",
    "    'Q': 'Q',\n",
    "    'R': 'R',\n",
    "    'S': 'S',\n",
    "    'T': 'T',\n",
    "    'U': 'U',\n",
    "    'V': 'V',\n",
    "    'W': 'W',\n",
    "    'X': 'X',\n",
    "    'Y': 'Y',\n",
    "    'Z': 'Z',\n",
    "    '0.0': '0',\n",
    "    '1.0': '1',\n",
    "    '2.0': '2',\n",
    "    '3.0': '3',\n",
    "    '4.0': '4',\n",
    "    '5.0': '5',\n",
    "    '6.0': '6',\n",
    "    '7.0': '7',\n",
    "    '8.0': '8',\n",
    "    '9.0': '9',\n",
    "}\n",
    "\n",
    "def get_landmarks(landmarks, is_right):\n",
    "    landmark_tensor = []\n",
    "\n",
    "    for i in range(21):\n",
    "        landmark_tensor += [\n",
    "            landmarks[i].x if is_right else 1 - landmarks[0].x,\n",
    "            landmarks[i].y,\n",
    "            landmarks[i].z\n",
    "        ]\n",
    "\n",
    "    return np.array(landmark_tensor).reshape(1, -1)\n",
    "\n",
    "def convert_prediction_to_text(prediction):\n",
    "    # Use the label to text mapping\n",
    "    return label_to_text.get(prediction[0][0], '')\n",
    "\n",
    "# Initialize variables for character accumulation\n",
    "current_word = \"\"\n",
    "current_sentence = \"\"\n",
    "selected_label = \"\"  # New variable for the selected label\n",
    "last_keypress_time = 0\n",
    "keypress_delay = 0.5  # Set the delay in seconds\n",
    "key_pressed = False  # Flag to track whether the key has been pressed for the current gesture\n",
    "\n",
    "# Capture video from the default camera (index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Use the Hands module from MediaPipe\n",
    "with mp.solutions.hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "        # Apply horizontal flip for a mirrored effect\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # Convert the image to RGB format\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image with the Hands module\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR format\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # If hands are detected in the image\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw landmarks and hand connections on the image\n",
    "                if DRAW_HANDS:\n",
    "                    mp.solutions.drawing_utils.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                        mp.solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "                # Check if the detected hand is the right hand\n",
    "                is_right = results.multi_handedness[0].classification[0].label == 'Right'\n",
    "\n",
    "                # Get the prediction for the hand landmarks\n",
    "                prediction = model.predict(get_landmarks(hand_landmarks.landmark, is_right))\n",
    "\n",
    "                # If a valid prediction is obtained\n",
    "                if len(prediction) == 1 and len(prediction[0]) == 1:\n",
    "                    # Convert the prediction to text\n",
    "                    text = convert_prediction_to_text(prediction)\n",
    "\n",
    "                    # Update the current word and sentence\n",
    "                    current_word += text\n",
    "                    current_sentence += text\n",
    "\n",
    "                    # Draw the text on the image\n",
    "                    cv2.putText(image, text, (45, 375), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 2, (255, 0, 0), 10)  # Warna biru untuk teks\n",
    "                    cv2.putText(image, text, (45, 375), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 2, (255, 255, 255), 5)  # Warna putih untuk teks\n",
    "\n",
    "                    # Check if the key has been pressed and update the selected label\n",
    "                    if text not in selected_label and not key_pressed:\n",
    "                        selected_label += text\n",
    "                        key_pressed = True\n",
    "                        # Tambahkan logika pemutaran suara di sini\n",
    "                        pygame.mixer.music.load(\"sound_effect.mp3\")\n",
    "                        pygame.mixer.music.play()\n",
    "\n",
    "        # Display the selected label on the image\n",
    "        cv2.putText(image, selected_label, (45, 450), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 10)\n",
    "        cv2.putText(image, selected_label, (45, 450), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 5)\n",
    "\n",
    "\n",
    "        # Display the image with the drawn annotations\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "        # Check for key events\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyWindow('MediaPipe Hands')\n",
    "            break\n",
    "        elif key == 13:  # Check for the 'Enter' key (key code 13)\n",
    "            key_pressed = False  # Reset the flag when the 'Enter' key is pressed\n",
    "            current_word += \" \"  # Add a space to separate words in the current sentence\n",
    "        elif key == 32:  # Check for the 'Space' key (key code 32)\n",
    "            current_sentence += selected_label + \" \"  # Add the selected label to the current sentence\n",
    "            selected_label = \"\"  # Reset the selected label\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Display the accumulated word and sentence\n",
    "# print(\"Current Word:\", current_word)\n",
    "print(\"Current Sentence:\", current_sentence)\n",
    "print(\"Selected Label:\", selected_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_myenv_kernel",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
